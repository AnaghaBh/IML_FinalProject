{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Loading the Dataset from Excel\n",
    "file_path = \"C:/Users/Anagha/Desktop/Training_Data_2.xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Print the columns of the DataFrame to understand its structure\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "# Ensure the dataset has 'text' and 'label' columns\n",
    "# Replace column names with the actual column names in your file\n",
    "df = df.rename(columns={\"cleaned_comment\": \"text\", \"Sentiment_Num\": \"label\"})\n",
    "\n",
    "# Drop rows where 'text' or 'label' are missing\n",
    "df = df.dropna(subset=[\"text\", \"label\"])\n",
    "\n",
    "# Remove rows with non-finite values in 'label'\n",
    "df = df[df[\"label\"].apply(lambda x: np.isfinite(x))]\n",
    "\n",
    "# Convert 'label' from float to integer type\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "# Check unique labels and data type\n",
    "print(\"Unique labels (after cleaning):\", df[\"label\"].unique())\n",
    "print(\"Label data type:\", df[\"label\"].dtype)\n",
    "\n",
    "# Convert the cleaned DataFrame to a Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenize the Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenization function to the dataset in batches\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# Split the dataset into training and testing sets (90% train, 10% test)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "test_dataset = split[\"test\"]\n",
    "\n",
    "\n",
    "print(\"Train dataset labels:\", train_dataset[\"label\"])\n",
    "print(\"Test dataset labels:\", test_dataset[\"label\"])\n",
    "\n",
    "# Remove unnecessary columns from the datasets to keep only the required data\n",
    "train_dataset = train_dataset.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "test_dataset = test_dataset.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "\n",
    "# Load the Pre-trained Model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=df[\"label\"].nunique())\n",
    "\n",
    "# Define Training Arguments and # Set up the training parameters\n",
    "training_args = TrainingArguments(\n",
    "    # Where to save model outputs and checkpoints\n",
    "    output_dir=\"./results\",\n",
    "    # Check model performance at the end of each training cycle\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # Save the model after each training cycle\n",
    "    save_strategy=\"epoch\",\n",
    "    # How fast the model learns\n",
    "    learning_rate=5e-5,\n",
    "    # Number of samples processed at once during training\n",
    "    per_device_train_batch_size=16,\n",
    "    # Number of samples processed at once during evaluation\n",
    "    per_device_eval_batch_size=16,\n",
    "    # Total number of times the model will see the training data\n",
    "    num_train_epochs=3,\n",
    "    # Regularization to prevent overfitting\n",
    "    weight_decay=0.01,\n",
    "    # Where to save logs of training progress\n",
    "    logging_dir=\"./logs\",\n",
    "    # How often to log training metrics\n",
    "    logging_steps=10,\n",
    "    # Use the best model found during training at the end\n",
    "    load_best_model_at_end=True,\n",
    "    # Metric used to determine the best model\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack predictions and true labels\n",
    "    logits, labels = eval_pred\n",
    "    # Get predicted class indices\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    # Calculate accuracy\n",
    "    return {\"accuracy\": (preds == labels).mean()}\n",
    "\n",
    "# Create a Trainer instance with the model, training settings, datasets, and metric function\n",
    "trainer = Trainer(\n",
    "    # The model to train\n",
    "    model=model,\n",
    "    # Training settings\n",
    "    args=training_args,\n",
    "    # Training data\n",
    "    train_dataset=train_dataset,\n",
    "    # Evaluation data\n",
    "    eval_dataset=test_dataset,\n",
    "    # Tokenizer for processing text\n",
    "    tokenizer=tokenizer,\n",
    "    # Function to calculate metrics\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Training the Model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluating the Model\n",
    "# Get predictions for the test data\n",
    "predictions = trainer.predict(test_dataset)\n",
    "# Determine predicted classes\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "print(\"Evaluation Metrics:\\n\", classification_report(test_dataset[\"label\"], preds))\n",
    "\n",
    "# Save the trained model and tokenizer for later use\n",
    "model.save_pretrained(\"fine_tuned_bert\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_bert\")\n",
    "\n",
    "# Using the Fine-Tuned Model for Inference\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline with the fine-tuned model\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=\"fine_tuned_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments,pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer that were saved earlier\n",
    "model_path = \"fine_tuned_bert\"  # Path to the saved fine-tuned model\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the New XLSX File with the actual testing data\n",
    "new_file_path = \"C:/Users/Anagha/Desktop/Youtube_Comments_Cleaned_Output.xlsx\"  # Replace with your file path\n",
    "new_df = pd.read_excel(new_file_path)\n",
    "\n",
    "# Ensure the DataFrame Has a 'text' Column\n",
    "# Replace 'cleaned_comment' with the actual column name in your new file\n",
    "new_df = new_df.rename(columns={\"cleaned_comment\": \"text\"})\n",
    "\n",
    "# Drop rows where 'text' is missing\n",
    "new_df = new_df.dropna(subset=[\"text\"])\n",
    "\n",
    "# Using the Sentiment Pipeline for Predictions\n",
    "# Create a sentiment analysis pipeline using the fine-tuned model and tokenizer\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    truncation=True,  # Add truncation to handle long sequences\n",
    "    max_length=512,   # Set the maximum length of input sequences(BERT Requirement)\n",
    ")\n",
    "\n",
    "#Function to split long comments into smaller chunks\n",
    "def split_long_comments(text, max_length=512):\n",
    "    #Splits long text into chunks smaller than max_length tokens.\n",
    "    tokens = tokenizer(text, truncation=False)[\"input_ids\"]\n",
    "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "# Apply splitting to long comments \n",
    "new_comments = new_df[\"text\"].tolist()  # Convert the 'text' column to a list\n",
    "# Convert the 'text' column to a list\n",
    "processed_comments = []\n",
    "# Check if comment exceeds the token limit\n",
    "for comment in new_comments:\n",
    "    if len(tokenizer(comment)[\"input_ids\"]) > 512:  # Check if comment exceeds token limit\n",
    "        processed_comments.extend(split_long_comments(comment))  # Split long comments\n",
    "    else:\n",
    "        processed_comments.append(comment)  # Add short comments as is\n",
    "\n",
    "# Get the sentiment predictions for the processed comments\n",
    "results = sentiment_pipeline(processed_comments)\n",
    "\n",
    "# Map results back to the original DataFrame\n",
    "# Create a list to store predicted labels for each comment\n",
    "predicted_labels = []\n",
    "# Track the current index in the results list\n",
    "current_index = 0\n",
    "\n",
    "# Loop through each original comment to assign predictions\n",
    "for comment in new_comments:\n",
    "    if len(tokenizer(comment)[\"input_ids\"]) > 512:\n",
    "        split_chunks = split_long_comments(comment)\n",
    "        # Get the predictions for these chunks from the results\n",
    "        chunk_predictions = results[current_index:current_index + len(split_chunks)]\n",
    "        # Combine chunk predictions \n",
    "        # Use majority voting to determine the final label for the long comment\n",
    "        # Count occurrences of each label\n",
    "        combined_label = max(set([pred[\"label\"] for pred in chunk_predictions]), key=lambda x: [pred[\"label\"] for pred in chunk_predictions].count(x))\n",
    "        # Add the combined label to the predicted labels list\n",
    "        predicted_labels.append(combined_label)\n",
    "        current_index += len(split_chunks)\n",
    "    else:\n",
    "        predicted_labels.append(results[current_index][\"label\"])\n",
    "        current_index += 1\n",
    "\n",
    "# Add the predicted labels as a new column in the DataFrame\n",
    "new_df[\"predicted_label\"] = predicted_labels\n",
    "\n",
    "#Saving the Results to a New Excel File\n",
    "output_file_path = \"C:/Users/Anagha/Desktop/Predicted_Test_Data2.xlsx\"  # Replace with desired output file path\n",
    "new_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Print a preview of the DataFrame\n",
    "print(new_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
